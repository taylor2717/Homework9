---
title: "homework9"
format: html
editor: visual
---



# Homework 9

## Homework 8

## Set up and Data Read



```{r}
# Packages
library(tidyverse)
library(lubridate)
library(janitor)
library(tidymodels)
library(skimr)
#library(GGally)

set.seed(2717)

# Read data
url <- "https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv"
bike_raw <- readr::read_csv(url, locale = readr::locale(encoding = "latin1"))

# Make names easy to work with
bike_raw <- bike_raw %>% clean_names()

```



## Data Checks and Basic EDA

Before building models, I began by examining the raw Seoul Bike data to confirm that values were complete, properly formatted, and sensible. This step ensures that later analysis and modeling are based on reliable inputs.

First, I loaded the dataset and used `glimpse()` and `skim()` to quickly inspect structure and missingness. The dataset contained 8,760 hourly observations across weather and time variables, and the `skim()` output confirmed **no missing values** in any column. All variables had the expected types — continuous for weather metrics and integer for counts.

Next, I **converted and cleaned data types** to make the dataset easier to work with:

-   The `Date` column was parsed into a true date object using `lubridate::dmy()`.

-   The categorical columns `Seasons`, `Holiday`, and `Functioning Day` were recoded as factors to reflect their discrete nature.

-   Within `Functioning Day`, the value `"NoFunc(Non Functional Hours)"` indicates non-operational hours, so I kept only rows marked `"Yes"` to focus on periods when bike rentals were possible.

-   I also standardized column names to **lowercase snake case** (e.g., `rented_bike_count` → `bike_count`, `wind_speed_m_s` → `windspeed_ms`, etc.) for cleaner syntax.

After cleaning, I verified the categorical variables with `unique()` to ensure that each had reasonable levels:

-   `seasons` included *Winter, Spring, Summer, Autumn*

-   `holiday` had *Holiday* and *No Holiday*

-   `functioning_day` had *Yes* and *No*

Finally, I checked **summary statistics** for all numeric variables (like temperature, humidity, visibility, windspeed, and solar radiation) to confirm that ranges were realistic for Seoul’s climate. The summary showed temperature values between roughly –30°C and 39°C, humidity mostly between 20–100%, and zero rainfall or snowfall for most hours — all reasonable. No variable appeared to contain outliers or structural errors.



```{r}
# Glimpse, missingness
glimpse(bike_raw)
skim(bike_raw)

# Coerce types
bike <- bike_raw %>%
  mutate(
    date = lubridate::dmy(date),
    seasons = factor(seasons, levels = c("Winter","Spring","Summer","Autumn")),
    holiday = factor(holiday),
    functioning_day = case_when(
      functioning_day %in% c("Yes","Fun(Functional hours)","Functional hours","Fun") ~ "Yes",
      TRUE ~ "No"
    ) %>% factor(levels = c("No","Yes"))
  ) %>%
  janitor::clean_names() %>%
  rename(
    bike_count = rented_bike_count,
    temp_c = temperature_c,
    windspeed_ms = wind_speed_m_s,
    dewpoint_c = dew_point_temperature_c,
    solar_mj = solar_radiation_mj_m2
  )

# Check categorical values
map(bike[c("seasons","holiday","functioning_day")], ~unique(.x))
summary(select(bike, where(is.numeric)))
```



## Aggregate to Daily Level

To simplify analysis, I summarized the hourly Seoul Bike data so that each day corresponds to a single observation. This makes it easier to model overall daily rental trends rather than hourly fluctuations.

I began by filtering to **Functioning Days = “Yes”**, since non-functional hours (as seen earlier) correspond to periods when rentals are not possible. Keeping these rows would artificially lower averages, so they were excluded.

Next, I used `group_by(date, seasons, holiday)` to create one row per day, per season, per holiday category. For each group:

-   I **summed** the `bike_count`, `rainfall_mm`, and `snowfall_cm` variables since these values accumulate throughout the day.

-   I **averaged** the remaining weather-related variables (`temp_c`, `humidity_percent`, `windspeed_ms`, `visibility_10m`, `dewpoint_c`, and `solar_mj`) to represent typical daily conditions.

This produced a new dataset (`bike_day`) with **353 daily observations**, each containing one overall snapshot of the day’s rentals and weather conditions.



```{r}
bike_day <- bike %>%
  filter(functioning_day == "Yes") %>%
  group_by(date, seasons, holiday) %>%
  summarise(
    bike_count   = sum(bike_count,   na.rm = TRUE),
    rainfall_mm  = sum(rainfall_mm,  na.rm = TRUE),
    snowfall_cm  = sum(snowfall_cm,  na.rm = TRUE),
    temp_c       = mean(temp_c,      na.rm = TRUE),
    humidity_percent     = mean(humidity_percent,    na.rm = TRUE),
    windspeed_ms = mean(windspeed_ms,na.rm = TRUE),
    visibility_10m = mean(visibility_10m, na.rm = TRUE),
    dewpoint_c   = mean(dewpoint_c,  na.rm = TRUE),
    solar_mj     = mean(solar_mj,    na.rm = TRUE),
    .groups = "drop"
  )
```



## Post-Aggregation EDA (summaries, correlations, quick plots)

After aggregation, I recreated summary statistics and visualizations to understand patterns in the cleaned daily dataset.

1.  Summaries Across Categories
    -   Using grouped summaries:
        -   By Season:
            -   Bike rentals peaked in **Summer and Autumn**, averaging roughly **24,000–22,000 rentals/day**, compared to only **\~5,000 in Winter**.
            -   Warmer temperatures and greater solar radiation clearly aligned with higher rental counts, while humidity and precipitation were lower in those seasons.
        -   By Holiday:
            -   Rentals were noticeably lower on **Holidays (\~12,700)** than **Non-Holidays (\~17,700)**, showing that daily commuter traffic likely drives much of the usage.
    -   These differences confirm that both **seasonal climate** and **day type** strongly influence rental volume.
2.  Correlations Between Numeric Variables
    -   I calculated pairwise correlations among all continuous variables to identify potential relationships.
    -   Key Findings:
        -   **Bike count** correlated **positively** with temperature (r ≈ 0.65) and solar radiation (r ≈ 0.55).
        -   It correlated **negatively** with humidity (r ≈ –0.23) and windspeed (r ≈ –0.19).
        -   These results are intuitive: warmer, sunnier days encourage riding, while humid or windy weather discourages it.
3.  Visual Exploration
    -   To complement the numeric summaries, I created two quick scatterplots:
        -   `bike_count vs temp_c` (colored by season) — showed a clear upward trend, with summer and autumn points clustered toward higher counts and temperatures.
        -   `bike_count vs solar_mj` (colored by holiday) — highlighted that, even at similar sunlight levels, rentals are lower on holidays.
    -   Together, these visuals illustrate strong seasonal and environmental effects on daily bike demand.



```{r}
# Summaries by season/holiday
bike_day %>%
  group_by(seasons) %>%
  summarise(across(c(bike_count, temp_c, humidity_percent, rainfall_mm, snowfall_cm, solar_mj), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))

bike_day %>%
  group_by(holiday) %>%
  summarise(across(c(bike_count, temp_c, humidity_percent), list(mean = mean, sd = sd), .names = "{.col}_{.fn}"))


# Correlations 
num_vars <- bike_day %>% dplyr::select(where(is.numeric))
stopifnot(nrow(num_vars) > 0, ncol(num_vars) > 1)
cor(num_vars, use = "pairwise.complete.obs")

# A couple of quick visuals 
ggplot(bike_day, aes(temp_c, bike_count, color = seasons)) + geom_point(alpha = 0.6)
ggplot(bike_day, aes(solar_mj, bike_count, color = holiday)) + geom_point(alpha = 0.6)

```



### Summary of EDA Findings

-   The dataset is now **clean, aggregated, and free of missingness**.

-   Daily rentals increase with **warmer temperatures and greater solar exposure**.

-   **Holidays and winter months** see much lower ridership.

-   Several predictors (temperature, solar radiation, humidity) show meaningful correlation with bike counts — providing a strong foundation for the upcoming modeling phase.

## Modeling (tidymodels)

To prepare for modeling, I split the daily dataset into **training (75%)** and **testing (25%)** subsets using `initial_split()` from the **rsample** package. The split was **stratified by season**, ensuring that each season was proportionally represented in both subsets. From the training set, I then created a **10-fold cross-validation** object (`vfold_cv()`) — also stratified by season — to support model tuning and selection.

### Train/Test Split (75/25), stratified by season



```{r}
set.seed(2717)
bike_split <- initial_split(bike_day, prop = 0.75, strata = seasons)
bike_train <- training(bike_split)
bike_test  <- testing(bike_split)

folds <- vfold_cv(bike_train, v = 10, strata = seasons)
```



### Shared helpers

We use a **linear regression model** specified with the `"lm"` engine (`linear_reg() %>% set_engine("lm")`).

Three different recipes were built to compare model performance.



```{r}
lm_spec <- linear_reg() %>% set_engine("lm")

# ---------- Recipe 1: baseline ----------
rec1 <- recipe(bike_count ~ ., data = bike_train) %>%
  step_date(date, features = "dow", label = TRUE) %>%               # makes date_dow
  step_mutate(day_type = factor(if_else(date_dow %in% c("Sat","Sun"),
                                        "weekend","weekday"))) %>%
  step_rm(date_dow, date) %>%                                       # drop intermediates
  step_normalize(all_numeric_predictors()) %>%                       # <- no hard-coded names
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_zv(all_predictors())

# ---------- Recipe 2: + interactions ----------
rec2 <- recipe(bike_count ~ ., data = bike_train) %>%
  step_date(date, features = "dow", label = TRUE) %>%
  step_mutate(day_type = factor(if_else(date_dow %in% c("Sat","Sun"),
                                        "weekend","weekday"))) %>%
  step_rm(date_dow, date) %>%
  # interactions on original vars; they’ll be handled when we dummy later
  step_interact(terms = ~ seasons:holiday + seasons:temp_c + temp_c:rainfall_mm) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_zv(all_predictors())

# ---------- Recipe 3: + quadratic terms on numeric predictors ----------
rec3 <- recipe(bike_count ~ ., data = bike_train) %>%
  step_date(date, features = "dow", label = TRUE) %>%
  step_mutate(day_type = factor(if_else(date_dow %in% c("Sat","Sun"),
                                        "weekend","weekday"))) %>%
  step_rm(date_dow, date) %>%
  step_interact(terms = ~ seasons:holiday + seasons:temp_c + temp_c:rainfall_mm) %>%
  step_poly(all_numeric_predictors(), degree = 2, options = list(raw = TRUE)) %>% # only numeric preds
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_zv(all_predictors())


```



### Workflows & 10-fold CV

Each recipe was paired with the same linear regression specification and evaluated using **10-fold cross-validation** on the training set.



```{r}
wf1 <- workflow() %>% add_model(lm_spec) %>% add_recipe(rec1)
wf2 <- workflow() %>% add_model(lm_spec) %>% add_recipe(rec2)
wf3 <- workflow() %>% add_model(lm_spec) %>% add_recipe(rec3)

metrics_set <- metric_set(rmse, rsq)

res1 <- fit_resamples(wf1, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))
res2 <- fit_resamples(wf2, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))
res3 <- fit_resamples(wf3, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))

cv_tbl <- bind_rows(
  collect_metrics(res1) %>% mutate(model = "Recipe 1"),
  collect_metrics(res2) %>% mutate(model = "Recipe 2"),
  collect_metrics(res3) %>% mutate(model = "Recipe 3")
)

cv_tbl %>% arrange(.metric, mean)
```



Based on the lowest average RMSE (≈ 2973), **Recipe 2** was selected as the best-performing model.

### Final Fit on Training + Test RMSE + Coefficients

The winning model was then refit on the **entire training data** using `last_fit()` and evaluated on the held-out test set.



```{r}
# choose the winner by lowest RMSE
cv_rmse <- cv_tbl %>% filter(.metric == "rmse") %>% arrange(mean)
winner <- cv_rmse$model[1]
winner
# Map name -> workflow
winner_wf <- list("Recipe 1" = wf1, "Recipe 2" = wf2, "Recipe 3" = wf3)[[winner]]

# last_fit gives test performance directly
final_fit <- last_fit(winner_wf, split = bike_split)

# Test RMSE
test_metrics <- collect_metrics(final_fit)
test_metrics  # includes rmse, rsq on the test set

# Final model coefficients (fit on full training set)
final_fit %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  arrange(desc(abs(estimate))) %>%
  print(n = 30)

```



These results show that the model generalizes reasonably well, explaining about **69% of the variation** in daily bike rentals on unseen data.

Extracting the final model coefficients using `extract_fit_parsnip()` and `tidy()` revealed key predictors. Overall, temperature, solar radiation, and humidity emerge as the most influential weather factors on daily bike demand, while seasonal and holiday effects adjust those relationships appropriately.

## Homework 9

### LASSO



```{r}
library(tidymodels)
library(baguette)     # bagged trees
library(vip)          # variable importance plots
library(rpart.plot)
library(glmnet)
```

```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

wf_lasso <- workflow() %>% add_model(lasso_spec) %>% add_recipe(rec1)

grid_lasso <- grid_regular(penalty(range = c(-4, 1)), levels = 30) # 10^-4 ... 10^1

res_lasso <- tune_grid(wf_lasso, resamples = folds,
                       grid = grid_lasso, metrics = metric_set(rmse, mae, rsq))

best_lasso <- select_best(res_lasso, metric = "rmse")
final_lasso <- finalize_workflow(wf_lasso, best_lasso)

last_lasso <- last_fit(final_lasso, split = bike_split, metrics = metric_set(rmse, mae, rsq))
metrics_lasso <- collect_metrics(last_lasso)   # includes rmse, rsq, mae
coef_lasso <- extract_fit_parsnip(last_lasso) %>% tidy()
```

```{r}
# --- LASSO coefficients ---
best_lambda <- best_lasso$penalty

coef_lasso <- last_lasso %>%
  extract_fit_parsnip() %>%
  tidy() %>%                       
  arrange(desc(abs(estimate)))

knitr::kable(
  coef_lasso,
  digits = 3,
  caption = paste0("LASSO: Final model coefficients at λ = ", signif(best_lambda, 3))
)

```



### Regression Tree



```{r}
tree_spec <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%
  set_engine("rpart") %>% set_mode("regression")

wf_tree <- workflow() %>% add_model(tree_spec) %>% add_recipe(rec1)

grid_tree <- grid_regular(cost_complexity(), tree_depth(range = c(2L, 20L)), min_n(),
                          levels = c(10, 10, 5))

res_tree <- tune_grid(wf_tree, resamples = folds, grid = grid_tree,
                      metrics = metric_set(rmse, mae, rsq))

best_tree <- select_best(res_tree, metric = "rmse")
final_tree <- finalize_workflow(wf_tree, best_tree)
last_tree <- last_fit(final_tree, split = bike_split, metrics = metric_set(rmse, mae, rsq))
metrics_tree <- collect_metrics(last_tree)

# Plot final tree
fit_tree <- extract_fit_engine(last_tree)  # rpart object
rpart.plot(fit_tree, main = "Final CART Tree", roundint=FALSE)

```



### Bagged Tree



```{r}
bag_spec <- bag_tree(min_n = tune()) %>% 
  set_engine("rpart", times = 50) %>%  # 50 bootstraps
  set_mode("regression")

wf_bag <- workflow() %>% add_model(bag_spec) %>% add_recipe(rec1)

grid_bag <- grid_regular(min_n(), levels = 6)

res_bag <- tune_grid(wf_bag, resamples = folds, grid = grid_bag,
                     metrics = metric_set(rmse, mae, rsq))

best_bag <- select_best(res_bag, metric = "rmse")
final_bag <- finalize_workflow(wf_bag, best_bag)
last_bag <- last_fit(final_bag, split = bike_split, metrics = metric_set(rmse, mae, rsq))
metrics_bag <- collect_metrics(last_bag)

# robust prediction wrapper for vip::vi_permute()
pred_fun <- function(object, new_data, newdata = NULL) {
  # handle vip versions that pass `newdata=`
  if (!is.null(newdata)) new_data <- newdata
  out <- predict(object, new_data = as.data.frame(new_data))
  # bagger returns a tibble with .pred; coerce safely
  if (is.data.frame(out)) out <- out[[1]]
  as.numeric(out)
}

# build the baked TRAIN set from your existing recipe/split
rec_prep <- prep(rec1, training = bike_train)
train_baked <- bake(rec_prep, new_data = bike_train)

train_x <- train_baked %>% dplyr::select(-bike_count)
train_y <- train_baked$bike_count

set.seed(2717)
vi_bag <- vip::vi_permute(
  object = extract_fit_parsnip(last_bag)$fit,  # bagger object
  train  = as.data.frame(train_x),             # predictors (baked)
  target = train_y,                            # response (baked)
  metric = "rmse",
  smaller_is_better = TRUE,
  nsim = 10,
  pred_wrapper = pred_fun
)

vip::vip(vi_bag) + ggplot2::ggtitle("Bagged Tree – Permutation Variable Importance")

```



### Random Forest



```{r}
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("regression")

wf_rf <- workflow() %>% add_model(rf_spec) %>% add_recipe(rec1)

# mtry grid uses number of predictors after recipe; use finalize to compute
prep_cols <- prep(rec1, training = bike_train) %>% juice() %>% select(-bike_count)
p <- ncol(prep_cols)

grid_rf <- grid_regular(
  mtry(range = c(2L, max(2L, floor(p*0.8)))),
  min_n(),
  levels = 6
)

res_rf <- tune_grid(wf_rf, resamples = folds, grid = grid_rf,
                    metrics = metric_set(rmse, mae, rsq))

best_rf <- select_best(res_rf, metric = "rmse")
final_rf <- finalize_workflow(wf_rf, best_rf)
last_rf <- last_fit(final_rf, split = bike_split, metrics = metric_set(rmse, mae, rsq))
metrics_rf <- collect_metrics(last_rf)

# Variable importance plot
fit_rf <- extract_fit_engine(last_rf)  # ranger
vip(fit_rf) + ggplot2::ggtitle("Random Forest – Permutation Variable Importance")


```



### Best MLR from HW8



```{r, warning=FALSE}
last_mlr <- last_fit(winner_wf, split = bike_split)
metrics_mlr <- collect_metrics(last_mlr)
coef_mlr <- extract_fit_parsnip(last_mlr) %>% tidy()

```

```{r}
library(broom)

# --- MLR coefficients (from lm) ---
coef_mlr <- last_mlr %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  arrange(desc(abs(estimate)))

knitr::kable(coef_mlr, digits = 3, caption = "MLR: Final model coefficients")

```



### Compare final models on the **test set** (RMSE + MAE)



```{r}
compare_tbl <- bind_rows(
  metrics_mlr  %>% mutate(model = "MLR (HW8 best)"),
  metrics_lasso%>% mutate(model = "LASSO"),
  metrics_tree %>% mutate(model = "CART"),
  metrics_bag  %>% mutate(model = "Bagged Tree"),
  metrics_rf   %>% mutate(model = "Random Forest")
) %>%
  # keep only rmse and mae
  filter(.metric %in% c("rmse", "mae")) %>%
  # ensure each model has both metrics (fill missing with NA)
  tidyr::complete(model, .metric, fill = list(.estimate = NA)) %>%
  select(model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  arrange(rmse)

compare_tbl


```



### Overall Best Model



```{r}
# Fit best overall to the ENTIRE dataset (train + test combined)
best_overall_fit <- fit(final_rf, data = bind_rows(bike_train, bike_test))
best_overall_fit

```

