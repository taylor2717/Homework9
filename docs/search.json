[
  {
    "objectID": "hw9.html",
    "href": "hw9.html",
    "title": "homework9",
    "section": "",
    "text": "# Packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(skimr)\n#library(GGally)\n\nset.seed(2717)\n\n# Read data\nurl &lt;- \"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\"\nbike_raw &lt;- readr::read_csv(url, locale = readr::locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make names easy to work with\nbike_raw &lt;- bike_raw %&gt;% clean_names()\n\n\n\n\nBefore building models, I began by examining the raw Seoul Bike data to confirm that values were complete, properly formatted, and sensible. This step ensures that later analysis and modeling are based on reliable inputs.\nFirst, I loaded the dataset and used glimpse() and skim() to quickly inspect structure and missingness. The dataset contained 8,760 hourly observations across weather and time variables, and the skim() output confirmed no missing values in any column. All variables had the expected types — continuous for weather metrics and integer for counts.\nNext, I converted and cleaned data types to make the dataset easier to work with:\n\nThe Date column was parsed into a true date object using lubridate::dmy().\nThe categorical columns Seasons, Holiday, and Functioning Day were recoded as factors to reflect their discrete nature.\nWithin Functioning Day, the value \"NoFunc(Non Functional Hours)\" indicates non-operational hours, so I kept only rows marked \"Yes\" to focus on periods when bike rentals were possible.\nI also standardized column names to lowercase snake case (e.g., rented_bike_count → bike_count, wind_speed_m_s → windspeed_ms, etc.) for cleaner syntax.\n\nAfter cleaning, I verified the categorical variables with unique() to ensure that each had reasonable levels:\n\nseasons included Winter, Spring, Summer, Autumn\nholiday had Holiday and No Holiday\nfunctioning_day had Yes and No\n\nFinally, I checked summary statistics for all numeric variables (like temperature, humidity, visibility, windspeed, and solar radiation) to confirm that ranges were realistic for Seoul’s climate. The summary showed temperature values between roughly –30°C and 39°C, humidity mostly between 20–100%, and zero rainfall or snowfall for most hours — all reasonable. No variable appeared to contain outliers or structural errors.\n\n# Glimpse, missingness\nglimpse(bike_raw)\n\nRows: 8,760\nColumns: 14\n$ date                    &lt;chr&gt; \"01/12/2017\", \"01/12/2017\", \"01/12/2017\", \"01/…\n$ rented_bike_count       &lt;dbl&gt; 254, 204, 173, 107, 78, 100, 181, 460, 930, 49…\n$ hour                    &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n$ temperature_c           &lt;dbl&gt; -5.2, -5.5, -6.0, -6.2, -6.0, -6.4, -6.6, -7.4…\n$ humidity_percent        &lt;dbl&gt; 37, 38, 39, 40, 36, 37, 35, 38, 37, 27, 24, 21…\n$ wind_speed_m_s          &lt;dbl&gt; 2.2, 0.8, 1.0, 0.9, 2.3, 1.5, 1.3, 0.9, 1.1, 0…\n$ visibility_10m          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000…\n$ dew_point_temperature_c &lt;dbl&gt; -17.6, -17.6, -17.7, -17.6, -18.6, -18.7, -19.…\n$ solar_radiation_mj_m2   &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ rainfall_mm             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ snowfall_cm             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ seasons                 &lt;chr&gt; \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"Winte…\n$ holiday                 &lt;chr&gt; \"No Holiday\", \"No Holiday\", \"No Holiday\", \"No …\n$ functioning_day         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes…\n\nskim(bike_raw)\n\n\nData summary\n\n\nName\nbike_raw\n\n\nNumber of rows\n8760\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndate\n0\n1\n10\n10\n0\n365\n0\n\n\nseasons\n0\n1\n6\n6\n0\n4\n0\n\n\nholiday\n0\n1\n7\n10\n0\n2\n0\n\n\nfunctioning_day\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrented_bike_count\n0\n1\n704.60\n645.00\n0.0\n191.00\n504.50\n1065.25\n3556.00\n▇▃▂▁▁\n\n\nhour\n0\n1\n11.50\n6.92\n0.0\n5.75\n11.50\n17.25\n23.00\n▇▇▆▇▇\n\n\ntemperature_c\n0\n1\n12.88\n11.94\n-17.8\n3.50\n13.70\n22.50\n39.40\n▂▆▆▇▂\n\n\nhumidity_percent\n0\n1\n58.23\n20.36\n0.0\n42.00\n57.00\n74.00\n98.00\n▁▅▇▇▅\n\n\nwind_speed_m_s\n0\n1\n1.72\n1.04\n0.0\n0.90\n1.50\n2.30\n7.40\n▇▇▂▁▁\n\n\nvisibility_10m\n0\n1\n1436.83\n608.30\n27.0\n940.00\n1698.00\n2000.00\n2000.00\n▂▂▂▂▇\n\n\ndew_point_temperature_c\n0\n1\n4.07\n13.06\n-30.6\n-4.70\n5.10\n14.80\n27.20\n▂▃▇▇▆\n\n\nsolar_radiation_mj_m2\n0\n1\n0.57\n0.87\n0.0\n0.00\n0.01\n0.93\n3.52\n▇▁▁▁▁\n\n\nrainfall_mm\n0\n1\n0.15\n1.13\n0.0\n0.00\n0.00\n0.00\n35.00\n▇▁▁▁▁\n\n\nsnowfall_cm\n0\n1\n0.08\n0.44\n0.0\n0.00\n0.00\n0.00\n8.80\n▇▁▁▁▁\n\n\n\n\n# Coerce types\nbike &lt;- bike_raw %&gt;%\n  mutate(\n    date = lubridate::dmy(date),\n    seasons = factor(seasons, levels = c(\"Winter\",\"Spring\",\"Summer\",\"Autumn\")),\n    holiday = factor(holiday),\n    functioning_day = case_when(\n      functioning_day %in% c(\"Yes\",\"Fun(Functional hours)\",\"Functional hours\",\"Fun\") ~ \"Yes\",\n      TRUE ~ \"No\"\n    ) %&gt;% factor(levels = c(\"No\",\"Yes\"))\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  rename(\n    bike_count = rented_bike_count,\n    temp_c = temperature_c,\n    windspeed_ms = wind_speed_m_s,\n    dewpoint_c = dew_point_temperature_c,\n    solar_mj = solar_radiation_mj_m2\n  )\n\n# Check categorical values\nmap(bike[c(\"seasons\",\"holiday\",\"functioning_day\")], ~unique(.x))\n\n$seasons\n[1] Winter Spring Summer Autumn\nLevels: Winter Spring Summer Autumn\n\n$holiday\n[1] No Holiday Holiday   \nLevels: Holiday No Holiday\n\n$functioning_day\n[1] Yes No \nLevels: No Yes\n\nsummary(select(bike, where(is.numeric)))\n\n   bike_count          hour           temp_c       humidity_percent\n Min.   :   0.0   Min.   : 0.00   Min.   :-17.80   Min.   : 0.00   \n 1st Qu.: 191.0   1st Qu.: 5.75   1st Qu.:  3.50   1st Qu.:42.00   \n Median : 504.5   Median :11.50   Median : 13.70   Median :57.00   \n Mean   : 704.6   Mean   :11.50   Mean   : 12.88   Mean   :58.23   \n 3rd Qu.:1065.2   3rd Qu.:17.25   3rd Qu.: 22.50   3rd Qu.:74.00   \n Max.   :3556.0   Max.   :23.00   Max.   : 39.40   Max.   :98.00   \n  windspeed_ms   visibility_10m   dewpoint_c         solar_mj     \n Min.   :0.000   Min.   :  27   Min.   :-30.600   Min.   :0.0000  \n 1st Qu.:0.900   1st Qu.: 940   1st Qu.: -4.700   1st Qu.:0.0000  \n Median :1.500   Median :1698   Median :  5.100   Median :0.0100  \n Mean   :1.725   Mean   :1437   Mean   :  4.074   Mean   :0.5691  \n 3rd Qu.:2.300   3rd Qu.:2000   3rd Qu.: 14.800   3rd Qu.:0.9300  \n Max.   :7.400   Max.   :2000   Max.   : 27.200   Max.   :3.5200  \n  rainfall_mm       snowfall_cm     \n Min.   : 0.0000   Min.   :0.00000  \n 1st Qu.: 0.0000   1st Qu.:0.00000  \n Median : 0.0000   Median :0.00000  \n Mean   : 0.1487   Mean   :0.07507  \n 3rd Qu.: 0.0000   3rd Qu.:0.00000  \n Max.   :35.0000   Max.   :8.80000  \n\n\n\n\n\nTo simplify analysis, I summarized the hourly Seoul Bike data so that each day corresponds to a single observation. This makes it easier to model overall daily rental trends rather than hourly fluctuations.\nI began by filtering to Functioning Days = “Yes”, since non-functional hours (as seen earlier) correspond to periods when rentals are not possible. Keeping these rows would artificially lower averages, so they were excluded.\nNext, I used group_by(date, seasons, holiday) to create one row per day, per season, per holiday category. For each group:\n\nI summed the bike_count, rainfall_mm, and snowfall_cm variables since these values accumulate throughout the day.\nI averaged the remaining weather-related variables (temp_c, humidity_percent, windspeed_ms, visibility_10m, dewpoint_c, and solar_mj) to represent typical daily conditions.\n\nThis produced a new dataset (bike_day) with 353 daily observations, each containing one overall snapshot of the day’s rentals and weather conditions.\n\nbike_day &lt;- bike %&gt;%\n  filter(functioning_day == \"Yes\") %&gt;%\n  group_by(date, seasons, holiday) %&gt;%\n  summarise(\n    bike_count   = sum(bike_count,   na.rm = TRUE),\n    rainfall_mm  = sum(rainfall_mm,  na.rm = TRUE),\n    snowfall_cm  = sum(snowfall_cm,  na.rm = TRUE),\n    temp_c       = mean(temp_c,      na.rm = TRUE),\n    humidity_percent     = mean(humidity_percent,    na.rm = TRUE),\n    windspeed_ms = mean(windspeed_ms,na.rm = TRUE),\n    visibility_10m = mean(visibility_10m, na.rm = TRUE),\n    dewpoint_c   = mean(dewpoint_c,  na.rm = TRUE),\n    solar_mj     = mean(solar_mj,    na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n\n\n\nAfter aggregation, I recreated summary statistics and visualizations to understand patterns in the cleaned daily dataset.\n\nSummaries Across Categories\n\nUsing grouped summaries:\n\nBy Season:\n\nBike rentals peaked in Summer and Autumn, averaging roughly 24,000–22,000 rentals/day, compared to only ~5,000 in Winter.\nWarmer temperatures and greater solar radiation clearly aligned with higher rental counts, while humidity and precipitation were lower in those seasons.\n\nBy Holiday:\n\nRentals were noticeably lower on Holidays (~12,700) than Non-Holidays (~17,700), showing that daily commuter traffic likely drives much of the usage.\n\n\nThese differences confirm that both seasonal climate and day type strongly influence rental volume.\n\nCorrelations Between Numeric Variables\n\nI calculated pairwise correlations among all continuous variables to identify potential relationships.\nKey Findings:\n\nBike count correlated positively with temperature (r ≈ 0.65) and solar radiation (r ≈ 0.55).\nIt correlated negatively with humidity (r ≈ –0.23) and windspeed (r ≈ –0.19).\nThese results are intuitive: warmer, sunnier days encourage riding, while humid or windy weather discourages it.\n\n\nVisual Exploration\n\nTo complement the numeric summaries, I created two quick scatterplots:\n\nbike_count vs temp_c (colored by season) — showed a clear upward trend, with summer and autumn points clustered toward higher counts and temperatures.\nbike_count vs solar_mj (colored by holiday) — highlighted that, even at similar sunlight levels, rentals are lower on holidays.\n\nTogether, these visuals illustrate strong seasonal and environmental effects on daily bike demand.\n\n\n\n# Summaries by season/holiday\nbike_day %&gt;%\n  group_by(seasons) %&gt;%\n  summarise(across(c(bike_count, temp_c, humidity_percent, rainfall_mm, snowfall_cm, solar_mj), list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 4 × 13\n  seasons bike_count_mean bike_count_sd temp_c_mean temp_c_sd\n  &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Winter            5413.         1808.       -2.54      4.79\n2 Spring           17910.         8357.       13.0       5.70\n3 Summer           24818.         7297.       26.6       3.64\n4 Autumn           22099.         6711.       13.8       6.57\n# ℹ 8 more variables: humidity_percent_mean &lt;dbl&gt;, humidity_percent_sd &lt;dbl&gt;,\n#   rainfall_mm_mean &lt;dbl&gt;, rainfall_mm_sd &lt;dbl&gt;, snowfall_cm_mean &lt;dbl&gt;,\n#   snowfall_cm_sd &lt;dbl&gt;, solar_mj_mean &lt;dbl&gt;, solar_mj_sd &lt;dbl&gt;\n\nbike_day %&gt;%\n  group_by(holiday) %&gt;%\n  summarise(across(c(bike_count, temp_c, humidity_percent), list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 2 × 7\n  holiday    bike_count_mean bike_count_sd temp_c_mean temp_c_sd\n  &lt;fct&gt;                &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Holiday             12700.        10504.        9.65      11.2\n2 No Holiday          17727.         9862.       12.9       11.7\n# ℹ 2 more variables: humidity_percent_mean &lt;dbl&gt;, humidity_percent_sd &lt;dbl&gt;\n\n# Correlations \nnum_vars &lt;- bike_day %&gt;% dplyr::select(where(is.numeric))\nstopifnot(nrow(num_vars) &gt; 0, ncol(num_vars) &gt; 1)\ncor(num_vars, use = \"pairwise.complete.obs\")\n\n                  bike_count rainfall_mm snowfall_cm       temp_c\nbike_count        1.00000000 -0.23910905 -0.26529110  0.753076732\nrainfall_mm      -0.23910905  1.00000000 -0.02313404  0.144517274\nsnowfall_cm      -0.26529110 -0.02313404  1.00000000 -0.266963662\ntemp_c            0.75307673  0.14451727 -0.26696366  1.000000000\nhumidity_percent  0.03588697  0.52864263  0.06539191  0.404167486\nwindspeed_ms     -0.19288142 -0.10167578  0.02088156 -0.260721792\nvisibility_10m    0.16599375 -0.22199387 -0.10188902  0.002336683\ndewpoint_c        0.65047655  0.26456621 -0.20955286  0.962796255\nsolar_mj          0.73589290 -0.32270413 -0.23343056  0.550274301\n                 humidity_percent windspeed_ms visibility_10m dewpoint_c\nbike_count             0.03588697  -0.19288142    0.165993749  0.6504765\nrainfall_mm            0.52864263  -0.10167578   -0.221993866  0.2645662\nsnowfall_cm            0.06539191   0.02088156   -0.101889019 -0.2095529\ntemp_c                 0.40416749  -0.26072179    0.002336683  0.9627963\nhumidity_percent       1.00000000  -0.23425778   -0.559177334  0.6320473\nwindspeed_ms          -0.23425778   1.00000000    0.206022636 -0.2877032\nvisibility_10m        -0.55917733   0.20602264    1.000000000 -0.1535516\ndewpoint_c             0.63204729  -0.28770322   -0.153551591  1.0000000\nsolar_mj              -0.27444967   0.09612635    0.271395906  0.3831571\n                    solar_mj\nbike_count        0.73589290\nrainfall_mm      -0.32270413\nsnowfall_cm      -0.23343056\ntemp_c            0.55027430\nhumidity_percent -0.27444967\nwindspeed_ms      0.09612635\nvisibility_10m    0.27139591\ndewpoint_c        0.38315713\nsolar_mj          1.00000000\n\n# A couple of quick visuals \nggplot(bike_day, aes(temp_c, bike_count, color = seasons)) + geom_point(alpha = 0.6)\n\n\n\n\n\n\n\nggplot(bike_day, aes(solar_mj, bike_count, color = holiday)) + geom_point(alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nThe dataset is now clean, aggregated, and free of missingness.\nDaily rentals increase with warmer temperatures and greater solar exposure.\nHolidays and winter months see much lower ridership.\nSeveral predictors (temperature, solar radiation, humidity) show meaningful correlation with bike counts — providing a strong foundation for the upcoming modeling phase.\n\n\n\n\n\nTo prepare for modeling, I split the daily dataset into training (75%) and testing (25%) subsets using initial_split() from the rsample package. The split was stratified by season, ensuring that each season was proportionally represented in both subsets. From the training set, I then created a 10-fold cross-validation object (vfold_cv()) — also stratified by season — to support model tuning and selection.\n\n\n\nset.seed(2717)\nbike_split &lt;- initial_split(bike_day, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test  &lt;- testing(bike_split)\n\nfolds &lt;- vfold_cv(bike_train, v = 10, strata = seasons)\n\n\n\n\nWe use a linear regression model specified with the \"lm\" engine (linear_reg() %&gt;% set_engine(\"lm\")).\nThree different recipes were built to compare model performance.\n\nlm_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# ---------- Recipe 1: baseline ----------\nrec1 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%               # makes date_dow\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\n                                        \"weekend\",\"weekday\"))) %&gt;%\n  step_rm(date_dow, date) %&gt;%                                       # drop intermediates\n  step_normalize(all_numeric_predictors()) %&gt;%                       # &lt;- no hard-coded names\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\n# ---------- Recipe 2: + interactions ----------\nrec2 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\n                                        \"weekend\",\"weekday\"))) %&gt;%\n  step_rm(date_dow, date) %&gt;%\n  # interactions on original vars; they’ll be handled when we dummy later\n  step_interact(terms = ~ seasons:holiday + seasons:temp_c + temp_c:rainfall_mm) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\n# ---------- Recipe 3: + quadratic terms on numeric predictors ----------\nrec3 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\n                                        \"weekend\",\"weekday\"))) %&gt;%\n  step_rm(date_dow, date) %&gt;%\n  step_interact(terms = ~ seasons:holiday + seasons:temp_c + temp_c:rainfall_mm) %&gt;%\n  step_poly(all_numeric_predictors(), degree = 2, options = list(raw = TRUE)) %&gt;% # only numeric preds\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\n\n\n\nEach recipe was paired with the same linear regression specification and evaluated using 10-fold cross-validation on the training set.\n\nwf1 &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec1)\nwf2 &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec2)\nwf3 &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec3)\n\nmetrics_set &lt;- metric_set(rmse, rsq)\n\nres1 &lt;- fit_resamples(wf1, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nres2 &lt;- fit_resamples(wf2, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nres3 &lt;- fit_resamples(wf3, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\ncv_tbl &lt;- bind_rows(\n  collect_metrics(res1) %&gt;% mutate(model = \"Recipe 1\"),\n  collect_metrics(res2) %&gt;% mutate(model = \"Recipe 2\"),\n  collect_metrics(res3) %&gt;% mutate(model = \"Recipe 3\")\n)\n\ncv_tbl %&gt;% arrange(.metric, mean)\n\n# A tibble: 6 × 7\n  .metric .estimator     mean     n  std_err .config              model   \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;   \n1 rmse    standard   2973.       10 244.     Preprocessor1_Model1 Recipe 2\n2 rmse    standard   3064.       10 243.     Preprocessor1_Model1 Recipe 3\n3 rmse    standard   4150.       10 214.     Preprocessor1_Model1 Recipe 1\n4 rsq     standard      0.831    10   0.0189 Preprocessor1_Model1 Recipe 1\n5 rsq     standard      0.906    10   0.0127 Preprocessor1_Model1 Recipe 3\n6 rsq     standard      0.910    10   0.0131 Preprocessor1_Model1 Recipe 2\n\n\nBased on the lowest average RMSE (≈ 2973), Recipe 2 was selected as the best-performing model.\n\n\n\nThe winning model was then refit on the entire training data using last_fit() and evaluated on the held-out test set.\n\n# choose the winner by lowest RMSE\ncv_rmse &lt;- cv_tbl %&gt;% filter(.metric == \"rmse\") %&gt;% arrange(mean)\nwinner &lt;- cv_rmse$model[1]\nwinner\n\n[1] \"Recipe 2\"\n\n# Map name -&gt; workflow\nwinner_wf &lt;- list(\"Recipe 1\" = wf1, \"Recipe 2\" = wf2, \"Recipe 3\" = wf3)[[winner]]\n\n# last_fit gives test performance directly\nfinal_fit &lt;- last_fit(winner_wf, split = bike_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n# Test RMSE\ntest_metrics &lt;- collect_metrics(final_fit)\ntest_metrics  # includes rmse, rsq on the test set\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    5946.    Preprocessor1_Model1\n2 rsq     standard       0.693 Preprocessor1_Model1\n\n# Final model coefficients (fit on full training set)\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  arrange(desc(abs(estimate))) %&gt;%\n  print(n = 30)\n\n# A tibble: 24 × 5\n   term                                estimate std.error statistic   p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                          21802.      1366.   16.0     1.04e-39\n 2 seasonsSummer_x_temp_c              -17478.      1599.  -10.9     6.94e-23\n 3 seasons_Spring                      -15316.      3131.   -4.89    1.82e- 6\n 4 `seasonsSummer_x_holidayNo Holiday`  14950.      1633.    9.15    2.34e-17\n 5 dewpoint_c                           12470.      4351.    2.87    4.52e- 3\n 6 seasons_Winter                       -9082.      2459.   -3.69    2.73e- 4\n 7 temp_c                               -5716.      3615.   -1.58    1.15e- 1\n 8 humidity_percent                     -3905.      1403.   -2.78    5.81e- 3\n 9 solar_mj                              3377.       328.   10.3     7.42e-21\n10 seasonsSpring_x_temp_c                2699.       690.    3.91    1.18e- 4\n11 holiday_Holiday                      -2645.      1098.   -2.41    1.68e- 2\n12 day_type_weekday                      2511.       388.    6.47    5.41e-10\n13 `seasonsSpring_x_holidayNo Holiday`   1713.      1053.    1.63    1.05e- 1\n14 temp_c_x_rainfall_mm                 -1607.       639.   -2.51    1.26e- 2\n15 rainfall_mm                          -1126.       664.   -1.70    9.11e- 2\n16 `seasonsAutumn_x_holidayNo Holiday`   -574.       969.   -0.592   5.55e- 1\n17 snowfall_cm                           -247.       194.   -1.27    2.05e- 1\n18 visibility_10m                         193.       263.    0.734   4.64e- 1\n19 windspeed_ms                          -143.       202.   -0.705   4.81e- 1\n20 seasonsAutumn_x_temp_c                  61.0      693.    0.0880  9.30e- 1\n21 seasons_Summer                          NA         NA    NA      NA       \n22 seasons_Autumn                          NA         NA    NA      NA       \n23 holiday_No.Holiday                      NA         NA    NA      NA       \n24 day_type_weekend                        NA         NA    NA      NA       \n\n\nThese results show that the model generalizes reasonably well, explaining about 69% of the variation in daily bike rentals on unseen data.\nExtracting the final model coefficients using extract_fit_parsnip() and tidy() revealed key predictors. Overall, temperature, solar radiation, and humidity emerge as the most influential weather factors on daily bike demand, while seasonal and holiday effects adjust those relationships appropriately.\n\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(baguette)     # bagged trees\nlibrary(vip)          # variable importance plots\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-10\n\n\n\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_engine(\"glmnet\")\n\nwf_lasso &lt;- workflow() %&gt;% add_model(lasso_spec) %&gt;% add_recipe(rec1)\n\ngrid_lasso &lt;- grid_regular(penalty(range = c(-4, 1)), levels = 30) # 10^-4 ... 10^1\n\nres_lasso &lt;- tune_grid(wf_lasso, resamples = folds,\n                       grid = grid_lasso, metrics = metric_set(rmse, mae, rsq))\n\nbest_lasso &lt;- select_best(res_lasso, metric = \"rmse\")\nfinal_lasso &lt;- finalize_workflow(wf_lasso, best_lasso)\n\nlast_lasso &lt;- last_fit(final_lasso, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_lasso &lt;- collect_metrics(last_lasso)   # includes rmse, rsq, mae\ncoef_lasso &lt;- extract_fit_parsnip(last_lasso) %&gt;% tidy()\n\n\n# --- LASSO coefficients ---\nbest_lambda &lt;- best_lasso$penalty\n\ncoef_lasso &lt;- last_lasso %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%                       \n  arrange(desc(abs(estimate)))\n\nknitr::kable(\n  coef_lasso,\n  digits = 3,\n  caption = paste0(\"LASSO: Final model coefficients at λ = \", signif(best_lambda, 3))\n)\n\n\nLASSO: Final model coefficients at λ = 1e-04\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n14811.251\n0\n\n\nseasons_Autumn\n5189.276\n0\n\n\nsolar_mj\n4169.884\n0\n\n\ndewpoint_c\n4096.105\n0\n\n\nholiday_Holiday\n-3173.139\n0\n\n\nseasons_Winter\n-2881.895\n0\n\n\nday_type_weekday\n2478.709\n0\n\n\nrainfall_mm\n-1939.616\n0\n\n\nseasons_Summer\n1421.675\n0\n\n\nhumidity_percent\n-774.704\n0\n\n\nwindspeed_ms\n-534.453\n0\n\n\nsnowfall_cm\n-398.225\n0\n\n\nvisibility_10m\n107.491\n0\n\n\nseasons_Spring\n-0.542\n0\n\n\nholiday_No.Holiday\n0.000\n0\n\n\nday_type_weekend\n0.000\n0\n\n\ntemp_c\n0.000\n0\n\n\n\n\n\n\n\n\n\ntree_spec &lt;- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;% set_mode(\"regression\")\n\nwf_tree &lt;- workflow() %&gt;% add_model(tree_spec) %&gt;% add_recipe(rec1)\n\ngrid_tree &lt;- grid_regular(cost_complexity(), tree_depth(range = c(2L, 20L)), min_n(),\n                          levels = c(10, 10, 5))\n\nres_tree &lt;- tune_grid(wf_tree, resamples = folds, grid = grid_tree,\n                      metrics = metric_set(rmse, mae, rsq))\n\nbest_tree &lt;- select_best(res_tree, metric = \"rmse\")\nfinal_tree &lt;- finalize_workflow(wf_tree, best_tree)\nlast_tree &lt;- last_fit(final_tree, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_tree &lt;- collect_metrics(last_tree)\n\n# Plot final tree\nfit_tree &lt;- extract_fit_engine(last_tree)  # rpart object\nrpart.plot(fit_tree, main = \"Final CART Tree\", roundint=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nbag_spec &lt;- bag_tree(min_n = tune()) %&gt;% \n  set_engine(\"rpart\", times = 50) %&gt;%  # 50 bootstraps\n  set_mode(\"regression\")\n\nwf_bag &lt;- workflow() %&gt;% add_model(bag_spec) %&gt;% add_recipe(rec1)\n\ngrid_bag &lt;- grid_regular(min_n(), levels = 6)\n\nres_bag &lt;- tune_grid(wf_bag, resamples = folds, grid = grid_bag,\n                     metrics = metric_set(rmse, mae, rsq))\n\nbest_bag &lt;- select_best(res_bag, metric = \"rmse\")\nfinal_bag &lt;- finalize_workflow(wf_bag, best_bag)\nlast_bag &lt;- last_fit(final_bag, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_bag &lt;- collect_metrics(last_bag)\n\n# robust prediction wrapper for vip::vi_permute()\npred_fun &lt;- function(object, new_data, newdata = NULL) {\n  # handle vip versions that pass `newdata=`\n  if (!is.null(newdata)) new_data &lt;- newdata\n  out &lt;- predict(object, new_data = as.data.frame(new_data))\n  # bagger returns a tibble with .pred; coerce safely\n  if (is.data.frame(out)) out &lt;- out[[1]]\n  as.numeric(out)\n}\n\n# build the baked TRAIN set from your existing recipe/split\nrec_prep &lt;- prep(rec1, training = bike_train)\ntrain_baked &lt;- bake(rec_prep, new_data = bike_train)\n\ntrain_x &lt;- train_baked %&gt;% dplyr::select(-bike_count)\ntrain_y &lt;- train_baked$bike_count\n\nset.seed(2717)\nvi_bag &lt;- vip::vi_permute(\n  object = extract_fit_parsnip(last_bag)$fit,  # bagger object\n  train  = as.data.frame(train_x),             # predictors (baked)\n  target = train_y,                            # response (baked)\n  metric = \"rmse\",\n  smaller_is_better = TRUE,\n  nsim = 10,\n  pred_wrapper = pred_fun\n)\n\nvip::vip(vi_bag) + ggplot2::ggtitle(\"Bagged Tree – Permutation Variable Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\nrf_spec &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"regression\")\n\nwf_rf &lt;- workflow() %&gt;% add_model(rf_spec) %&gt;% add_recipe(rec1)\n\n# mtry grid uses number of predictors after recipe; use finalize to compute\nprep_cols &lt;- prep(rec1, training = bike_train) %&gt;% juice() %&gt;% select(-bike_count)\np &lt;- ncol(prep_cols)\n\ngrid_rf &lt;- grid_regular(\n  mtry(range = c(2L, max(2L, floor(p*0.8)))),\n  min_n(),\n  levels = 6\n)\n\nres_rf &lt;- tune_grid(wf_rf, resamples = folds, grid = grid_rf,\n                    metrics = metric_set(rmse, mae, rsq))\n\nbest_rf &lt;- select_best(res_rf, metric = \"rmse\")\nfinal_rf &lt;- finalize_workflow(wf_rf, best_rf)\nlast_rf &lt;- last_fit(final_rf, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_rf &lt;- collect_metrics(last_rf)\n\n# Variable importance plot\nfit_rf &lt;- extract_fit_engine(last_rf)  # ranger\nvip(fit_rf) + ggplot2::ggtitle(\"Random Forest – Permutation Variable Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\nlast_mlr &lt;- last_fit(winner_wf, split = bike_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\nmetrics_mlr &lt;- collect_metrics(last_mlr)\ncoef_mlr &lt;- extract_fit_parsnip(last_mlr) %&gt;% tidy()\n\n\nlibrary(broom)\n\n# --- MLR coefficients (from lm) ---\ncoef_mlr &lt;- last_mlr %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  arrange(desc(abs(estimate)))\n\nknitr::kable(coef_mlr, digits = 3, caption = \"MLR: Final model coefficients\")\n\n\nMLR: Final model coefficients\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n21802.215\n1365.997\n15.961\n0.000\n\n\nseasonsSummer_x_temp_c\n-17478.320\n1598.873\n-10.932\n0.000\n\n\nseasons_Spring\n-15316.402\n3131.129\n-4.892\n0.000\n\n\nseasonsSummer_x_holidayNo Holiday\n14949.578\n1633.349\n9.153\n0.000\n\n\ndewpoint_c\n12469.653\n4351.230\n2.866\n0.005\n\n\nseasons_Winter\n-9081.940\n2458.960\n-3.693\n0.000\n\n\ntemp_c\n-5716.079\n3615.453\n-1.581\n0.115\n\n\nhumidity_percent\n-3904.643\n1402.918\n-2.783\n0.006\n\n\nsolar_mj\n3377.110\n328.107\n10.293\n0.000\n\n\nseasonsSpring_x_temp_c\n2699.081\n689.513\n3.914\n0.000\n\n\nholiday_Holiday\n-2644.746\n1098.300\n-2.408\n0.017\n\n\nday_type_weekday\n2511.254\n388.244\n6.468\n0.000\n\n\nseasonsSpring_x_holidayNo Holiday\n1712.995\n1052.964\n1.627\n0.105\n\n\ntemp_c_x_rainfall_mm\n-1607.466\n639.177\n-2.515\n0.013\n\n\nrainfall_mm\n-1126.215\n663.826\n-1.697\n0.091\n\n\nseasonsAutumn_x_holidayNo Holiday\n-573.550\n969.316\n-0.592\n0.555\n\n\nsnowfall_cm\n-246.897\n194.191\n-1.271\n0.205\n\n\nvisibility_10m\n192.678\n262.534\n0.734\n0.464\n\n\nwindspeed_ms\n-142.676\n202.313\n-0.705\n0.481\n\n\nseasonsAutumn_x_temp_c\n61.043\n693.405\n0.088\n0.930\n\n\nseasons_Summer\nNA\nNA\nNA\nNA\n\n\nseasons_Autumn\nNA\nNA\nNA\nNA\n\n\nholiday_No.Holiday\nNA\nNA\nNA\nNA\n\n\nday_type_weekend\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\ncompare_tbl &lt;- bind_rows(\n  metrics_mlr  %&gt;% mutate(model = \"MLR (HW8 best)\"),\n  metrics_lasso%&gt;% mutate(model = \"LASSO\"),\n  metrics_tree %&gt;% mutate(model = \"CART\"),\n  metrics_bag  %&gt;% mutate(model = \"Bagged Tree\"),\n  metrics_rf   %&gt;% mutate(model = \"Random Forest\")\n) %&gt;%\n  # keep only rmse and mae\n  filter(.metric %in% c(\"rmse\", \"mae\")) %&gt;%\n  # ensure each model has both metrics (fill missing with NA)\n  tidyr::complete(model, .metric, fill = list(.estimate = NA)) %&gt;%\n  select(model, .metric, .estimate) %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  arrange(rmse)\n\ncompare_tbl\n\n# A tibble: 5 × 3\n  model            mae  rmse\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 Random Forest  2037. 2764.\n2 Bagged Tree    2056. 2778.\n3 CART           2668. 3481.\n4 LASSO          3129. 4084.\n5 MLR (HW8 best)   NA  5946.\n\n\n\n\n\n\n# Fit best overall to the ENTIRE dataset (train + test combined)\nbest_overall_fit &lt;- fit(final_rf, data = bind_rows(bike_train, bike_test))\nbest_overall_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_normalize()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~10L,      x), num.trees = ~1000, min.node.size = min_rows(~2L, x),      importance = ~\"permutation\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      353 \nNumber of independent variables:  16 \nMtry:                             10 \nTarget node size:                 2 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       7149854 \nR squared (OOB):                  0.9275944"
  },
  {
    "objectID": "hw9.html#set-up-and-data-read",
    "href": "hw9.html#set-up-and-data-read",
    "title": "homework9",
    "section": "",
    "text": "# Packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(skimr)\n#library(GGally)\n\nset.seed(2717)\n\n# Read data\nurl &lt;- \"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\"\nbike_raw &lt;- readr::read_csv(url, locale = readr::locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Make names easy to work with\nbike_raw &lt;- bike_raw %&gt;% clean_names()"
  },
  {
    "objectID": "hw9.html#data-checks-and-basic-eda",
    "href": "hw9.html#data-checks-and-basic-eda",
    "title": "homework9",
    "section": "",
    "text": "Before building models, I began by examining the raw Seoul Bike data to confirm that values were complete, properly formatted, and sensible. This step ensures that later analysis and modeling are based on reliable inputs.\nFirst, I loaded the dataset and used glimpse() and skim() to quickly inspect structure and missingness. The dataset contained 8,760 hourly observations across weather and time variables, and the skim() output confirmed no missing values in any column. All variables had the expected types — continuous for weather metrics and integer for counts.\nNext, I converted and cleaned data types to make the dataset easier to work with:\n\nThe Date column was parsed into a true date object using lubridate::dmy().\nThe categorical columns Seasons, Holiday, and Functioning Day were recoded as factors to reflect their discrete nature.\nWithin Functioning Day, the value \"NoFunc(Non Functional Hours)\" indicates non-operational hours, so I kept only rows marked \"Yes\" to focus on periods when bike rentals were possible.\nI also standardized column names to lowercase snake case (e.g., rented_bike_count → bike_count, wind_speed_m_s → windspeed_ms, etc.) for cleaner syntax.\n\nAfter cleaning, I verified the categorical variables with unique() to ensure that each had reasonable levels:\n\nseasons included Winter, Spring, Summer, Autumn\nholiday had Holiday and No Holiday\nfunctioning_day had Yes and No\n\nFinally, I checked summary statistics for all numeric variables (like temperature, humidity, visibility, windspeed, and solar radiation) to confirm that ranges were realistic for Seoul’s climate. The summary showed temperature values between roughly –30°C and 39°C, humidity mostly between 20–100%, and zero rainfall or snowfall for most hours — all reasonable. No variable appeared to contain outliers or structural errors.\n\n# Glimpse, missingness\nglimpse(bike_raw)\n\nRows: 8,760\nColumns: 14\n$ date                    &lt;chr&gt; \"01/12/2017\", \"01/12/2017\", \"01/12/2017\", \"01/…\n$ rented_bike_count       &lt;dbl&gt; 254, 204, 173, 107, 78, 100, 181, 460, 930, 49…\n$ hour                    &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n$ temperature_c           &lt;dbl&gt; -5.2, -5.5, -6.0, -6.2, -6.0, -6.4, -6.6, -7.4…\n$ humidity_percent        &lt;dbl&gt; 37, 38, 39, 40, 36, 37, 35, 38, 37, 27, 24, 21…\n$ wind_speed_m_s          &lt;dbl&gt; 2.2, 0.8, 1.0, 0.9, 2.3, 1.5, 1.3, 0.9, 1.1, 0…\n$ visibility_10m          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000…\n$ dew_point_temperature_c &lt;dbl&gt; -17.6, -17.6, -17.7, -17.6, -18.6, -18.7, -19.…\n$ solar_radiation_mj_m2   &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ rainfall_mm             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ snowfall_cm             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ seasons                 &lt;chr&gt; \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"Winte…\n$ holiday                 &lt;chr&gt; \"No Holiday\", \"No Holiday\", \"No Holiday\", \"No …\n$ functioning_day         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes…\n\nskim(bike_raw)\n\n\nData summary\n\n\nName\nbike_raw\n\n\nNumber of rows\n8760\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndate\n0\n1\n10\n10\n0\n365\n0\n\n\nseasons\n0\n1\n6\n6\n0\n4\n0\n\n\nholiday\n0\n1\n7\n10\n0\n2\n0\n\n\nfunctioning_day\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrented_bike_count\n0\n1\n704.60\n645.00\n0.0\n191.00\n504.50\n1065.25\n3556.00\n▇▃▂▁▁\n\n\nhour\n0\n1\n11.50\n6.92\n0.0\n5.75\n11.50\n17.25\n23.00\n▇▇▆▇▇\n\n\ntemperature_c\n0\n1\n12.88\n11.94\n-17.8\n3.50\n13.70\n22.50\n39.40\n▂▆▆▇▂\n\n\nhumidity_percent\n0\n1\n58.23\n20.36\n0.0\n42.00\n57.00\n74.00\n98.00\n▁▅▇▇▅\n\n\nwind_speed_m_s\n0\n1\n1.72\n1.04\n0.0\n0.90\n1.50\n2.30\n7.40\n▇▇▂▁▁\n\n\nvisibility_10m\n0\n1\n1436.83\n608.30\n27.0\n940.00\n1698.00\n2000.00\n2000.00\n▂▂▂▂▇\n\n\ndew_point_temperature_c\n0\n1\n4.07\n13.06\n-30.6\n-4.70\n5.10\n14.80\n27.20\n▂▃▇▇▆\n\n\nsolar_radiation_mj_m2\n0\n1\n0.57\n0.87\n0.0\n0.00\n0.01\n0.93\n3.52\n▇▁▁▁▁\n\n\nrainfall_mm\n0\n1\n0.15\n1.13\n0.0\n0.00\n0.00\n0.00\n35.00\n▇▁▁▁▁\n\n\nsnowfall_cm\n0\n1\n0.08\n0.44\n0.0\n0.00\n0.00\n0.00\n8.80\n▇▁▁▁▁\n\n\n\n\n# Coerce types\nbike &lt;- bike_raw %&gt;%\n  mutate(\n    date = lubridate::dmy(date),\n    seasons = factor(seasons, levels = c(\"Winter\",\"Spring\",\"Summer\",\"Autumn\")),\n    holiday = factor(holiday),\n    functioning_day = case_when(\n      functioning_day %in% c(\"Yes\",\"Fun(Functional hours)\",\"Functional hours\",\"Fun\") ~ \"Yes\",\n      TRUE ~ \"No\"\n    ) %&gt;% factor(levels = c(\"No\",\"Yes\"))\n  ) %&gt;%\n  janitor::clean_names() %&gt;%\n  rename(\n    bike_count = rented_bike_count,\n    temp_c = temperature_c,\n    windspeed_ms = wind_speed_m_s,\n    dewpoint_c = dew_point_temperature_c,\n    solar_mj = solar_radiation_mj_m2\n  )\n\n# Check categorical values\nmap(bike[c(\"seasons\",\"holiday\",\"functioning_day\")], ~unique(.x))\n\n$seasons\n[1] Winter Spring Summer Autumn\nLevels: Winter Spring Summer Autumn\n\n$holiday\n[1] No Holiday Holiday   \nLevels: Holiday No Holiday\n\n$functioning_day\n[1] Yes No \nLevels: No Yes\n\nsummary(select(bike, where(is.numeric)))\n\n   bike_count          hour           temp_c       humidity_percent\n Min.   :   0.0   Min.   : 0.00   Min.   :-17.80   Min.   : 0.00   \n 1st Qu.: 191.0   1st Qu.: 5.75   1st Qu.:  3.50   1st Qu.:42.00   \n Median : 504.5   Median :11.50   Median : 13.70   Median :57.00   \n Mean   : 704.6   Mean   :11.50   Mean   : 12.88   Mean   :58.23   \n 3rd Qu.:1065.2   3rd Qu.:17.25   3rd Qu.: 22.50   3rd Qu.:74.00   \n Max.   :3556.0   Max.   :23.00   Max.   : 39.40   Max.   :98.00   \n  windspeed_ms   visibility_10m   dewpoint_c         solar_mj     \n Min.   :0.000   Min.   :  27   Min.   :-30.600   Min.   :0.0000  \n 1st Qu.:0.900   1st Qu.: 940   1st Qu.: -4.700   1st Qu.:0.0000  \n Median :1.500   Median :1698   Median :  5.100   Median :0.0100  \n Mean   :1.725   Mean   :1437   Mean   :  4.074   Mean   :0.5691  \n 3rd Qu.:2.300   3rd Qu.:2000   3rd Qu.: 14.800   3rd Qu.:0.9300  \n Max.   :7.400   Max.   :2000   Max.   : 27.200   Max.   :3.5200  \n  rainfall_mm       snowfall_cm     \n Min.   : 0.0000   Min.   :0.00000  \n 1st Qu.: 0.0000   1st Qu.:0.00000  \n Median : 0.0000   Median :0.00000  \n Mean   : 0.1487   Mean   :0.07507  \n 3rd Qu.: 0.0000   3rd Qu.:0.00000  \n Max.   :35.0000   Max.   :8.80000"
  },
  {
    "objectID": "hw9.html#aggregate-to-daily-level",
    "href": "hw9.html#aggregate-to-daily-level",
    "title": "homework9",
    "section": "",
    "text": "To simplify analysis, I summarized the hourly Seoul Bike data so that each day corresponds to a single observation. This makes it easier to model overall daily rental trends rather than hourly fluctuations.\nI began by filtering to Functioning Days = “Yes”, since non-functional hours (as seen earlier) correspond to periods when rentals are not possible. Keeping these rows would artificially lower averages, so they were excluded.\nNext, I used group_by(date, seasons, holiday) to create one row per day, per season, per holiday category. For each group:\n\nI summed the bike_count, rainfall_mm, and snowfall_cm variables since these values accumulate throughout the day.\nI averaged the remaining weather-related variables (temp_c, humidity_percent, windspeed_ms, visibility_10m, dewpoint_c, and solar_mj) to represent typical daily conditions.\n\nThis produced a new dataset (bike_day) with 353 daily observations, each containing one overall snapshot of the day’s rentals and weather conditions.\n\nbike_day &lt;- bike %&gt;%\n  filter(functioning_day == \"Yes\") %&gt;%\n  group_by(date, seasons, holiday) %&gt;%\n  summarise(\n    bike_count   = sum(bike_count,   na.rm = TRUE),\n    rainfall_mm  = sum(rainfall_mm,  na.rm = TRUE),\n    snowfall_cm  = sum(snowfall_cm,  na.rm = TRUE),\n    temp_c       = mean(temp_c,      na.rm = TRUE),\n    humidity_percent     = mean(humidity_percent,    na.rm = TRUE),\n    windspeed_ms = mean(windspeed_ms,na.rm = TRUE),\n    visibility_10m = mean(visibility_10m, na.rm = TRUE),\n    dewpoint_c   = mean(dewpoint_c,  na.rm = TRUE),\n    solar_mj     = mean(solar_mj,    na.rm = TRUE),\n    .groups = \"drop\"\n  )"
  },
  {
    "objectID": "hw9.html#post-aggregation-eda-summaries-correlations-quick-plots",
    "href": "hw9.html#post-aggregation-eda-summaries-correlations-quick-plots",
    "title": "homework9",
    "section": "",
    "text": "After aggregation, I recreated summary statistics and visualizations to understand patterns in the cleaned daily dataset.\n\nSummaries Across Categories\n\nUsing grouped summaries:\n\nBy Season:\n\nBike rentals peaked in Summer and Autumn, averaging roughly 24,000–22,000 rentals/day, compared to only ~5,000 in Winter.\nWarmer temperatures and greater solar radiation clearly aligned with higher rental counts, while humidity and precipitation were lower in those seasons.\n\nBy Holiday:\n\nRentals were noticeably lower on Holidays (~12,700) than Non-Holidays (~17,700), showing that daily commuter traffic likely drives much of the usage.\n\n\nThese differences confirm that both seasonal climate and day type strongly influence rental volume.\n\nCorrelations Between Numeric Variables\n\nI calculated pairwise correlations among all continuous variables to identify potential relationships.\nKey Findings:\n\nBike count correlated positively with temperature (r ≈ 0.65) and solar radiation (r ≈ 0.55).\nIt correlated negatively with humidity (r ≈ –0.23) and windspeed (r ≈ –0.19).\nThese results are intuitive: warmer, sunnier days encourage riding, while humid or windy weather discourages it.\n\n\nVisual Exploration\n\nTo complement the numeric summaries, I created two quick scatterplots:\n\nbike_count vs temp_c (colored by season) — showed a clear upward trend, with summer and autumn points clustered toward higher counts and temperatures.\nbike_count vs solar_mj (colored by holiday) — highlighted that, even at similar sunlight levels, rentals are lower on holidays.\n\nTogether, these visuals illustrate strong seasonal and environmental effects on daily bike demand.\n\n\n\n# Summaries by season/holiday\nbike_day %&gt;%\n  group_by(seasons) %&gt;%\n  summarise(across(c(bike_count, temp_c, humidity_percent, rainfall_mm, snowfall_cm, solar_mj), list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 4 × 13\n  seasons bike_count_mean bike_count_sd temp_c_mean temp_c_sd\n  &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Winter            5413.         1808.       -2.54      4.79\n2 Spring           17910.         8357.       13.0       5.70\n3 Summer           24818.         7297.       26.6       3.64\n4 Autumn           22099.         6711.       13.8       6.57\n# ℹ 8 more variables: humidity_percent_mean &lt;dbl&gt;, humidity_percent_sd &lt;dbl&gt;,\n#   rainfall_mm_mean &lt;dbl&gt;, rainfall_mm_sd &lt;dbl&gt;, snowfall_cm_mean &lt;dbl&gt;,\n#   snowfall_cm_sd &lt;dbl&gt;, solar_mj_mean &lt;dbl&gt;, solar_mj_sd &lt;dbl&gt;\n\nbike_day %&gt;%\n  group_by(holiday) %&gt;%\n  summarise(across(c(bike_count, temp_c, humidity_percent), list(mean = mean, sd = sd), .names = \"{.col}_{.fn}\"))\n\n# A tibble: 2 × 7\n  holiday    bike_count_mean bike_count_sd temp_c_mean temp_c_sd\n  &lt;fct&gt;                &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Holiday             12700.        10504.        9.65      11.2\n2 No Holiday          17727.         9862.       12.9       11.7\n# ℹ 2 more variables: humidity_percent_mean &lt;dbl&gt;, humidity_percent_sd &lt;dbl&gt;\n\n# Correlations \nnum_vars &lt;- bike_day %&gt;% dplyr::select(where(is.numeric))\nstopifnot(nrow(num_vars) &gt; 0, ncol(num_vars) &gt; 1)\ncor(num_vars, use = \"pairwise.complete.obs\")\n\n                  bike_count rainfall_mm snowfall_cm       temp_c\nbike_count        1.00000000 -0.23910905 -0.26529110  0.753076732\nrainfall_mm      -0.23910905  1.00000000 -0.02313404  0.144517274\nsnowfall_cm      -0.26529110 -0.02313404  1.00000000 -0.266963662\ntemp_c            0.75307673  0.14451727 -0.26696366  1.000000000\nhumidity_percent  0.03588697  0.52864263  0.06539191  0.404167486\nwindspeed_ms     -0.19288142 -0.10167578  0.02088156 -0.260721792\nvisibility_10m    0.16599375 -0.22199387 -0.10188902  0.002336683\ndewpoint_c        0.65047655  0.26456621 -0.20955286  0.962796255\nsolar_mj          0.73589290 -0.32270413 -0.23343056  0.550274301\n                 humidity_percent windspeed_ms visibility_10m dewpoint_c\nbike_count             0.03588697  -0.19288142    0.165993749  0.6504765\nrainfall_mm            0.52864263  -0.10167578   -0.221993866  0.2645662\nsnowfall_cm            0.06539191   0.02088156   -0.101889019 -0.2095529\ntemp_c                 0.40416749  -0.26072179    0.002336683  0.9627963\nhumidity_percent       1.00000000  -0.23425778   -0.559177334  0.6320473\nwindspeed_ms          -0.23425778   1.00000000    0.206022636 -0.2877032\nvisibility_10m        -0.55917733   0.20602264    1.000000000 -0.1535516\ndewpoint_c             0.63204729  -0.28770322   -0.153551591  1.0000000\nsolar_mj              -0.27444967   0.09612635    0.271395906  0.3831571\n                    solar_mj\nbike_count        0.73589290\nrainfall_mm      -0.32270413\nsnowfall_cm      -0.23343056\ntemp_c            0.55027430\nhumidity_percent -0.27444967\nwindspeed_ms      0.09612635\nvisibility_10m    0.27139591\ndewpoint_c        0.38315713\nsolar_mj          1.00000000\n\n# A couple of quick visuals \nggplot(bike_day, aes(temp_c, bike_count, color = seasons)) + geom_point(alpha = 0.6)\n\n\n\n\n\n\n\nggplot(bike_day, aes(solar_mj, bike_count, color = holiday)) + geom_point(alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nThe dataset is now clean, aggregated, and free of missingness.\nDaily rentals increase with warmer temperatures and greater solar exposure.\nHolidays and winter months see much lower ridership.\nSeveral predictors (temperature, solar radiation, humidity) show meaningful correlation with bike counts — providing a strong foundation for the upcoming modeling phase."
  },
  {
    "objectID": "hw9.html#modeling-tidymodels",
    "href": "hw9.html#modeling-tidymodels",
    "title": "homework9",
    "section": "",
    "text": "To prepare for modeling, I split the daily dataset into training (75%) and testing (25%) subsets using initial_split() from the rsample package. The split was stratified by season, ensuring that each season was proportionally represented in both subsets. From the training set, I then created a 10-fold cross-validation object (vfold_cv()) — also stratified by season — to support model tuning and selection.\n\n\n\nset.seed(2717)\nbike_split &lt;- initial_split(bike_day, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test  &lt;- testing(bike_split)\n\nfolds &lt;- vfold_cv(bike_train, v = 10, strata = seasons)\n\n\n\n\nWe use a linear regression model specified with the \"lm\" engine (linear_reg() %&gt;% set_engine(\"lm\")).\nThree different recipes were built to compare model performance.\n\nlm_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# ---------- Recipe 1: baseline ----------\nrec1 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%               # makes date_dow\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\n                                        \"weekend\",\"weekday\"))) %&gt;%\n  step_rm(date_dow, date) %&gt;%                                       # drop intermediates\n  step_normalize(all_numeric_predictors()) %&gt;%                       # &lt;- no hard-coded names\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\n# ---------- Recipe 2: + interactions ----------\nrec2 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\n                                        \"weekend\",\"weekday\"))) %&gt;%\n  step_rm(date_dow, date) %&gt;%\n  # interactions on original vars; they’ll be handled when we dummy later\n  step_interact(terms = ~ seasons:holiday + seasons:temp_c + temp_c:rainfall_mm) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\n# ---------- Recipe 3: + quadratic terms on numeric predictors ----------\nrec3 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\n                                        \"weekend\",\"weekday\"))) %&gt;%\n  step_rm(date_dow, date) %&gt;%\n  step_interact(terms = ~ seasons:holiday + seasons:temp_c + temp_c:rainfall_mm) %&gt;%\n  step_poly(all_numeric_predictors(), degree = 2, options = list(raw = TRUE)) %&gt;% # only numeric preds\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%\n  step_zv(all_predictors())\n\n\n\n\nEach recipe was paired with the same linear regression specification and evaluated using 10-fold cross-validation on the training set.\n\nwf1 &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec1)\nwf2 &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec2)\nwf3 &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec3)\n\nmetrics_set &lt;- metric_set(rmse, rsq)\n\nres1 &lt;- fit_resamples(wf1, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nres2 &lt;- fit_resamples(wf2, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nres3 &lt;- fit_resamples(wf3, resamples = folds, metrics = metrics_set, control = control_resamples(save_pred = TRUE))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\ncv_tbl &lt;- bind_rows(\n  collect_metrics(res1) %&gt;% mutate(model = \"Recipe 1\"),\n  collect_metrics(res2) %&gt;% mutate(model = \"Recipe 2\"),\n  collect_metrics(res3) %&gt;% mutate(model = \"Recipe 3\")\n)\n\ncv_tbl %&gt;% arrange(.metric, mean)\n\n# A tibble: 6 × 7\n  .metric .estimator     mean     n  std_err .config              model   \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;   \n1 rmse    standard   2973.       10 244.     Preprocessor1_Model1 Recipe 2\n2 rmse    standard   3064.       10 243.     Preprocessor1_Model1 Recipe 3\n3 rmse    standard   4150.       10 214.     Preprocessor1_Model1 Recipe 1\n4 rsq     standard      0.831    10   0.0189 Preprocessor1_Model1 Recipe 1\n5 rsq     standard      0.906    10   0.0127 Preprocessor1_Model1 Recipe 3\n6 rsq     standard      0.910    10   0.0131 Preprocessor1_Model1 Recipe 2\n\n\nBased on the lowest average RMSE (≈ 2973), Recipe 2 was selected as the best-performing model.\n\n\n\nThe winning model was then refit on the entire training data using last_fit() and evaluated on the held-out test set.\n\n# choose the winner by lowest RMSE\ncv_rmse &lt;- cv_tbl %&gt;% filter(.metric == \"rmse\") %&gt;% arrange(mean)\nwinner &lt;- cv_rmse$model[1]\nwinner\n\n[1] \"Recipe 2\"\n\n# Map name -&gt; workflow\nwinner_wf &lt;- list(\"Recipe 1\" = wf1, \"Recipe 2\" = wf2, \"Recipe 3\" = wf3)[[winner]]\n\n# last_fit gives test performance directly\nfinal_fit &lt;- last_fit(winner_wf, split = bike_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n# Test RMSE\ntest_metrics &lt;- collect_metrics(final_fit)\ntest_metrics  # includes rmse, rsq on the test set\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    5946.    Preprocessor1_Model1\n2 rsq     standard       0.693 Preprocessor1_Model1\n\n# Final model coefficients (fit on full training set)\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  arrange(desc(abs(estimate))) %&gt;%\n  print(n = 30)\n\n# A tibble: 24 × 5\n   term                                estimate std.error statistic   p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                          21802.      1366.   16.0     1.04e-39\n 2 seasonsSummer_x_temp_c              -17478.      1599.  -10.9     6.94e-23\n 3 seasons_Spring                      -15316.      3131.   -4.89    1.82e- 6\n 4 `seasonsSummer_x_holidayNo Holiday`  14950.      1633.    9.15    2.34e-17\n 5 dewpoint_c                           12470.      4351.    2.87    4.52e- 3\n 6 seasons_Winter                       -9082.      2459.   -3.69    2.73e- 4\n 7 temp_c                               -5716.      3615.   -1.58    1.15e- 1\n 8 humidity_percent                     -3905.      1403.   -2.78    5.81e- 3\n 9 solar_mj                              3377.       328.   10.3     7.42e-21\n10 seasonsSpring_x_temp_c                2699.       690.    3.91    1.18e- 4\n11 holiday_Holiday                      -2645.      1098.   -2.41    1.68e- 2\n12 day_type_weekday                      2511.       388.    6.47    5.41e-10\n13 `seasonsSpring_x_holidayNo Holiday`   1713.      1053.    1.63    1.05e- 1\n14 temp_c_x_rainfall_mm                 -1607.       639.   -2.51    1.26e- 2\n15 rainfall_mm                          -1126.       664.   -1.70    9.11e- 2\n16 `seasonsAutumn_x_holidayNo Holiday`   -574.       969.   -0.592   5.55e- 1\n17 snowfall_cm                           -247.       194.   -1.27    2.05e- 1\n18 visibility_10m                         193.       263.    0.734   4.64e- 1\n19 windspeed_ms                          -143.       202.   -0.705   4.81e- 1\n20 seasonsAutumn_x_temp_c                  61.0      693.    0.0880  9.30e- 1\n21 seasons_Summer                          NA         NA    NA      NA       \n22 seasons_Autumn                          NA         NA    NA      NA       \n23 holiday_No.Holiday                      NA         NA    NA      NA       \n24 day_type_weekend                        NA         NA    NA      NA       \n\n\nThese results show that the model generalizes reasonably well, explaining about 69% of the variation in daily bike rentals on unseen data.\nExtracting the final model coefficients using extract_fit_parsnip() and tidy() revealed key predictors. Overall, temperature, solar radiation, and humidity emerge as the most influential weather factors on daily bike demand, while seasonal and holiday effects adjust those relationships appropriately."
  },
  {
    "objectID": "hw9.html#homework-9-1",
    "href": "hw9.html#homework-9-1",
    "title": "homework9",
    "section": "",
    "text": "library(tidymodels)\nlibrary(baguette)     # bagged trees\nlibrary(vip)          # variable importance plots\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-10\n\n\n\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_engine(\"glmnet\")\n\nwf_lasso &lt;- workflow() %&gt;% add_model(lasso_spec) %&gt;% add_recipe(rec1)\n\ngrid_lasso &lt;- grid_regular(penalty(range = c(-4, 1)), levels = 30) # 10^-4 ... 10^1\n\nres_lasso &lt;- tune_grid(wf_lasso, resamples = folds,\n                       grid = grid_lasso, metrics = metric_set(rmse, mae, rsq))\n\nbest_lasso &lt;- select_best(res_lasso, metric = \"rmse\")\nfinal_lasso &lt;- finalize_workflow(wf_lasso, best_lasso)\n\nlast_lasso &lt;- last_fit(final_lasso, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_lasso &lt;- collect_metrics(last_lasso)   # includes rmse, rsq, mae\ncoef_lasso &lt;- extract_fit_parsnip(last_lasso) %&gt;% tidy()\n\n\n# --- LASSO coefficients ---\nbest_lambda &lt;- best_lasso$penalty\n\ncoef_lasso &lt;- last_lasso %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%                       \n  arrange(desc(abs(estimate)))\n\nknitr::kable(\n  coef_lasso,\n  digits = 3,\n  caption = paste0(\"LASSO: Final model coefficients at λ = \", signif(best_lambda, 3))\n)\n\n\nLASSO: Final model coefficients at λ = 1e-04\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n14811.251\n0\n\n\nseasons_Autumn\n5189.276\n0\n\n\nsolar_mj\n4169.884\n0\n\n\ndewpoint_c\n4096.105\n0\n\n\nholiday_Holiday\n-3173.139\n0\n\n\nseasons_Winter\n-2881.895\n0\n\n\nday_type_weekday\n2478.709\n0\n\n\nrainfall_mm\n-1939.616\n0\n\n\nseasons_Summer\n1421.675\n0\n\n\nhumidity_percent\n-774.704\n0\n\n\nwindspeed_ms\n-534.453\n0\n\n\nsnowfall_cm\n-398.225\n0\n\n\nvisibility_10m\n107.491\n0\n\n\nseasons_Spring\n-0.542\n0\n\n\nholiday_No.Holiday\n0.000\n0\n\n\nday_type_weekend\n0.000\n0\n\n\ntemp_c\n0.000\n0\n\n\n\n\n\n\n\n\n\ntree_spec &lt;- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;% set_mode(\"regression\")\n\nwf_tree &lt;- workflow() %&gt;% add_model(tree_spec) %&gt;% add_recipe(rec1)\n\ngrid_tree &lt;- grid_regular(cost_complexity(), tree_depth(range = c(2L, 20L)), min_n(),\n                          levels = c(10, 10, 5))\n\nres_tree &lt;- tune_grid(wf_tree, resamples = folds, grid = grid_tree,\n                      metrics = metric_set(rmse, mae, rsq))\n\nbest_tree &lt;- select_best(res_tree, metric = \"rmse\")\nfinal_tree &lt;- finalize_workflow(wf_tree, best_tree)\nlast_tree &lt;- last_fit(final_tree, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_tree &lt;- collect_metrics(last_tree)\n\n# Plot final tree\nfit_tree &lt;- extract_fit_engine(last_tree)  # rpart object\nrpart.plot(fit_tree, main = \"Final CART Tree\", roundint=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nbag_spec &lt;- bag_tree(min_n = tune()) %&gt;% \n  set_engine(\"rpart\", times = 50) %&gt;%  # 50 bootstraps\n  set_mode(\"regression\")\n\nwf_bag &lt;- workflow() %&gt;% add_model(bag_spec) %&gt;% add_recipe(rec1)\n\ngrid_bag &lt;- grid_regular(min_n(), levels = 6)\n\nres_bag &lt;- tune_grid(wf_bag, resamples = folds, grid = grid_bag,\n                     metrics = metric_set(rmse, mae, rsq))\n\nbest_bag &lt;- select_best(res_bag, metric = \"rmse\")\nfinal_bag &lt;- finalize_workflow(wf_bag, best_bag)\nlast_bag &lt;- last_fit(final_bag, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_bag &lt;- collect_metrics(last_bag)\n\n# robust prediction wrapper for vip::vi_permute()\npred_fun &lt;- function(object, new_data, newdata = NULL) {\n  # handle vip versions that pass `newdata=`\n  if (!is.null(newdata)) new_data &lt;- newdata\n  out &lt;- predict(object, new_data = as.data.frame(new_data))\n  # bagger returns a tibble with .pred; coerce safely\n  if (is.data.frame(out)) out &lt;- out[[1]]\n  as.numeric(out)\n}\n\n# build the baked TRAIN set from your existing recipe/split\nrec_prep &lt;- prep(rec1, training = bike_train)\ntrain_baked &lt;- bake(rec_prep, new_data = bike_train)\n\ntrain_x &lt;- train_baked %&gt;% dplyr::select(-bike_count)\ntrain_y &lt;- train_baked$bike_count\n\nset.seed(2717)\nvi_bag &lt;- vip::vi_permute(\n  object = extract_fit_parsnip(last_bag)$fit,  # bagger object\n  train  = as.data.frame(train_x),             # predictors (baked)\n  target = train_y,                            # response (baked)\n  metric = \"rmse\",\n  smaller_is_better = TRUE,\n  nsim = 10,\n  pred_wrapper = pred_fun\n)\n\nvip::vip(vi_bag) + ggplot2::ggtitle(\"Bagged Tree – Permutation Variable Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\nrf_spec &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"regression\")\n\nwf_rf &lt;- workflow() %&gt;% add_model(rf_spec) %&gt;% add_recipe(rec1)\n\n# mtry grid uses number of predictors after recipe; use finalize to compute\nprep_cols &lt;- prep(rec1, training = bike_train) %&gt;% juice() %&gt;% select(-bike_count)\np &lt;- ncol(prep_cols)\n\ngrid_rf &lt;- grid_regular(\n  mtry(range = c(2L, max(2L, floor(p*0.8)))),\n  min_n(),\n  levels = 6\n)\n\nres_rf &lt;- tune_grid(wf_rf, resamples = folds, grid = grid_rf,\n                    metrics = metric_set(rmse, mae, rsq))\n\nbest_rf &lt;- select_best(res_rf, metric = \"rmse\")\nfinal_rf &lt;- finalize_workflow(wf_rf, best_rf)\nlast_rf &lt;- last_fit(final_rf, split = bike_split, metrics = metric_set(rmse, mae, rsq))\nmetrics_rf &lt;- collect_metrics(last_rf)\n\n# Variable importance plot\nfit_rf &lt;- extract_fit_engine(last_rf)  # ranger\nvip(fit_rf) + ggplot2::ggtitle(\"Random Forest – Permutation Variable Importance\")\n\n\n\n\n\n\n\n\n\n\n\n\nlast_mlr &lt;- last_fit(winner_wf, split = bike_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\nmetrics_mlr &lt;- collect_metrics(last_mlr)\ncoef_mlr &lt;- extract_fit_parsnip(last_mlr) %&gt;% tidy()\n\n\nlibrary(broom)\n\n# --- MLR coefficients (from lm) ---\ncoef_mlr &lt;- last_mlr %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  arrange(desc(abs(estimate)))\n\nknitr::kable(coef_mlr, digits = 3, caption = \"MLR: Final model coefficients\")\n\n\nMLR: Final model coefficients\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n21802.215\n1365.997\n15.961\n0.000\n\n\nseasonsSummer_x_temp_c\n-17478.320\n1598.873\n-10.932\n0.000\n\n\nseasons_Spring\n-15316.402\n3131.129\n-4.892\n0.000\n\n\nseasonsSummer_x_holidayNo Holiday\n14949.578\n1633.349\n9.153\n0.000\n\n\ndewpoint_c\n12469.653\n4351.230\n2.866\n0.005\n\n\nseasons_Winter\n-9081.940\n2458.960\n-3.693\n0.000\n\n\ntemp_c\n-5716.079\n3615.453\n-1.581\n0.115\n\n\nhumidity_percent\n-3904.643\n1402.918\n-2.783\n0.006\n\n\nsolar_mj\n3377.110\n328.107\n10.293\n0.000\n\n\nseasonsSpring_x_temp_c\n2699.081\n689.513\n3.914\n0.000\n\n\nholiday_Holiday\n-2644.746\n1098.300\n-2.408\n0.017\n\n\nday_type_weekday\n2511.254\n388.244\n6.468\n0.000\n\n\nseasonsSpring_x_holidayNo Holiday\n1712.995\n1052.964\n1.627\n0.105\n\n\ntemp_c_x_rainfall_mm\n-1607.466\n639.177\n-2.515\n0.013\n\n\nrainfall_mm\n-1126.215\n663.826\n-1.697\n0.091\n\n\nseasonsAutumn_x_holidayNo Holiday\n-573.550\n969.316\n-0.592\n0.555\n\n\nsnowfall_cm\n-246.897\n194.191\n-1.271\n0.205\n\n\nvisibility_10m\n192.678\n262.534\n0.734\n0.464\n\n\nwindspeed_ms\n-142.676\n202.313\n-0.705\n0.481\n\n\nseasonsAutumn_x_temp_c\n61.043\n693.405\n0.088\n0.930\n\n\nseasons_Summer\nNA\nNA\nNA\nNA\n\n\nseasons_Autumn\nNA\nNA\nNA\nNA\n\n\nholiday_No.Holiday\nNA\nNA\nNA\nNA\n\n\nday_type_weekend\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\ncompare_tbl &lt;- bind_rows(\n  metrics_mlr  %&gt;% mutate(model = \"MLR (HW8 best)\"),\n  metrics_lasso%&gt;% mutate(model = \"LASSO\"),\n  metrics_tree %&gt;% mutate(model = \"CART\"),\n  metrics_bag  %&gt;% mutate(model = \"Bagged Tree\"),\n  metrics_rf   %&gt;% mutate(model = \"Random Forest\")\n) %&gt;%\n  # keep only rmse and mae\n  filter(.metric %in% c(\"rmse\", \"mae\")) %&gt;%\n  # ensure each model has both metrics (fill missing with NA)\n  tidyr::complete(model, .metric, fill = list(.estimate = NA)) %&gt;%\n  select(model, .metric, .estimate) %&gt;%\n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;%\n  arrange(rmse)\n\ncompare_tbl\n\n# A tibble: 5 × 3\n  model            mae  rmse\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 Random Forest  2037. 2764.\n2 Bagged Tree    2056. 2778.\n3 CART           2668. 3481.\n4 LASSO          3129. 4084.\n5 MLR (HW8 best)   NA  5946.\n\n\n\n\n\n\n# Fit best overall to the ENTIRE dataset (train + test combined)\nbest_overall_fit &lt;- fit(final_rf, data = bind_rows(bike_train, bike_test))\nbest_overall_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_normalize()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~10L,      x), num.trees = ~1000, min.node.size = min_rows(~2L, x),      importance = ~\"permutation\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      353 \nNumber of independent variables:  16 \nMtry:                             10 \nTarget node size:                 2 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       7149854 \nR squared (OOB):                  0.9275944"
  }
]